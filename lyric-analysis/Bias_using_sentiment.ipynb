{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1037
    },
    "colab_type": "code",
    "id": "ATcD69hlMNS_",
    "outputId": "381b9c6c-edb9-400a-d23c-4f6bfa9d27d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /Users/saurabhsharma/anaconda3/lib/python3.6/site-packages (1.5.0)\n",
      "Requirement already satisfied: pillow in /Users/saurabhsharma/anaconda3/lib/python3.6/site-packages (from wordcloud) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/saurabhsharma/anaconda3/lib/python3.6/site-packages (from wordcloud) (1.15.4)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: senticnet in /Users/saurabhsharma/anaconda3/lib/python3.6/site-packages (1.3)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting afinn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/e5/ffbb7ee3cca21ac6d310ac01944fb163c20030b45bda25421d725d8a859a/afinn-0.1.tar.gz (52kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: afinn\n",
      "  Running setup.py bdist_wheel for afinn ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/saurabhsharma/Library/Caches/pip/wheels/b5/1c/de/428301f3333ca509dcf20ff358690eb23a1388fbcbbde008b2\n",
      "Successfully built afinn\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: afinn\n",
      "Successfully installed afinn-0.1\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting twython\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/2b/c0883f05b03a8e87787d56395d6c89515fe7e0bf80abd3778b6bb3a6873f/twython-3.7.0.tar.gz\n",
      "Requirement already satisfied: requests>=2.1.0 in /Users/saurabhsharma/anaconda3/lib/python3.6/site-packages (from twython) (2.18.4)\n",
      "Collecting requests_oauthlib>=0.4.0 (from twython)\n",
      "  Downloading https://files.pythonhosted.org/packages/94/e7/c250d122992e1561690d9c0f7856dadb79d61fd4bdd0e598087dce607f6c/requests_oauthlib-1.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/saurabhsharma/anaconda3/lib/python3.6/site-packages (from requests>=2.1.0->twython) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/saurabhsharma/anaconda3/lib/python3.6/site-packages (from requests>=2.1.0->twython) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/saurabhsharma/anaconda3/lib/python3.6/site-packages (from requests>=2.1.0->twython) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saurabhsharma/anaconda3/lib/python3.6/site-packages (from requests>=2.1.0->twython) (2018.4.16)\n",
      "Collecting oauthlib>=0.6.2 (from requests_oauthlib>=0.4.0->twython)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/d1/ddd9cfea3e736399b97ded5c2dd62d1322adef4a72d816f1ed1049d6a179/oauthlib-2.1.0-py2.py3-none-any.whl (121kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 2.3MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: twython\n",
      "  Running setup.py bdist_wheel for twython ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/saurabhsharma/Library/Caches/pip/wheels/c2/b0/a3/5c4b4b87b8c9e4d99f1494a0b471f0134a74e5fb33d426d009\n",
      "Successfully built twython\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: oauthlib, requests-oauthlib, twython\n",
      "Successfully installed oauthlib-2.1.0 requests-oauthlib-1.0.0 twython-3.7.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting langdetect\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.0MB 3.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/saurabhsharma/anaconda3/lib/python3.6/site-packages (from langdetect) (1.11.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Running setup.py bdist_wheel for langdetect ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/saurabhsharma/Library/Caches/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
      "Successfully built langdetect\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.7\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saurabhsharma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/saurabhsharma/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Error loading senticnet: Package 'senticnet' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Error loading demo_liu_hu_lexicon: Package\n",
      "[nltk_data]     'demo_liu_hu_lexicon' not found in index\n",
      "[nltk_data] Error loading SentimentIntensityAnalyzer: Package\n",
      "[nltk_data]     'SentimentIntensityAnalyzer' not found in index\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/saurabhsharma/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "!pip install senticnet\n",
    "!pip install afinn\n",
    "!pip install twython\n",
    "!pip install langdetect\n",
    "# !pip install cPickle\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('senticnet')\n",
    "nltk.download('demo_liu_hu_lexicon')\n",
    "nltk.download('SentimentIntensityAnalyzer')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJtlrCH6WDYL"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import treebank\n",
    "from nltk import bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k8228aZDiatZ"
   },
   "outputs": [],
   "source": [
    "from senticnet.senticnet import SenticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1XniQA3tUP0w"
   },
   "source": [
    "Loading all sentiment based lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1VUADzL6MLfp"
   },
   "outputs": [],
   "source": [
    "import senticnet\n",
    "from afinn import Afinn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.sentiment.util import demo_liu_hu_lexicon\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "# import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aj-D_PmFVdXA"
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jQnbhNVuQpY3"
   },
   "outputs": [],
   "source": [
    "# HI Lex\n",
    "with open('lexicons/HI_POS_dict.json', 'r') as f:\n",
    "    pos_hi_dict = json.load(f)\n",
    "with open('lexicons/HI_NEG_dict.json', 'r') as f:\n",
    "    neg_hi_dict = json.load(f)\n",
    "\n",
    "# EmoSentiNet\n",
    "with open('lexicons/ESN_anger_dict.json', 'r') as f:\n",
    "    esn_anger_dict = json.load(f)\n",
    "with open('lexicons/ESN_sadness_dict.json', 'r') as f:\n",
    "    esn_sad_dict = json.load(f)\n",
    "with open('lexicons/ESN_joy_dict.json', 'r') as f:\n",
    "    esn_joy_dict = json.load(f)\n",
    "# with open('ESN_surprise_dict.json', 'r') as f:\n",
    "#     esn_surprise_dict = json.load(f)\n",
    "with open('lexicons/ESN_disgust_dict.json', 'r') as f:\n",
    "    esn_disgust_dict = json.load(f)\n",
    "with open('lexicons/ESN_fear_dict.json', 'r') as f:\n",
    "    esn_fear_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "347qmy68g_Jo"
   },
   "outputs": [],
   "source": [
    "def clean_dict(dict):\n",
    "  temp = {}\n",
    "  for k,v in dict.items():\n",
    "    cleaned = k.split('#')[0]\n",
    "    temp[cleaned] = v\n",
    "  return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qi4b_SrehmLU"
   },
   "outputs": [],
   "source": [
    "pos_hi_dict = clean_dict(pos_hi_dict)\n",
    "neg_hi_dict = clean_dict(neg_hi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XvpjvYvygll6"
   },
   "outputs": [],
   "source": [
    "def esn(s):\n",
    "\tbg = bigrams(s)\n",
    "\ttg = trigrams(s)\n",
    "\tscore_esn_sad = 0\n",
    "\tscore_esn_joy = 0\n",
    "\tscore_esn_anger = 0\n",
    "\tscore_esn_disgust = 0\n",
    "\tscore_esn_fear = 0\n",
    "\tfor b,t in zip(bg, tg):\n",
    "\t\tstring1 = b[0] + \" \" + b[1]\n",
    "\t\tstring2 = t[0] + \" \" + t[1] + \" \" + t[2]\n",
    "\t\tif string1 in esn_anger_dict.keys():\n",
    "\t\t\tscore_esn_anger += 1\n",
    "\t\tif string2 in esn_anger_dict.keys():\n",
    "\t\t\tscore_esn_anger += 1\n",
    "\t\t\n",
    "\t\tif string1 in esn_disgust_dict.keys():\n",
    "\t\t\tscore_esn_disgust += 1\n",
    "\t\tif string2 in esn_disgust_dict.keys():\n",
    "\t\t\tscore_esn_disgust += 1\n",
    "\t\t\n",
    "\t\tif string1 in esn_sad_dict.keys():\n",
    "\t\t\tscore_esn_sad += 1\n",
    "\t\tif string2 in esn_sad_dict.keys():\n",
    "\t\t\tscore_esn_sad += 1\n",
    "\n",
    "\t\tif string1 in esn_joy_dict.keys():\n",
    "\t\t\tscore_esn_joy += 1\n",
    "\t\tif string2 in esn_joy_dict.keys():\n",
    "\t\t\tscore_esn_joy += 1\n",
    "\n",
    "\t\tif string1 in esn_fear_dict.keys():\n",
    "\t\t\tscore_esn_fear += 1\n",
    "\t\tif string2 in esn_fear_dict.keys():\n",
    "\t\t\tscore_esn_fear += 1\n",
    "\treturn (score_esn_anger, score_esn_disgust, score_esn_sad, score_esn_joy, score_esn_fear)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tUbl7ed3glfE"
   },
   "outputs": [],
   "source": [
    "def liu_hu_lexicon(sentence):\n",
    "    \"\"\"\n",
    "    Basic example of sentiment classification using Liu and Hu opinion lexicon.\n",
    "    This function simply counts the number of positive, negative and neutral words\n",
    "    in the sentence and classifies it depending on which polarity is more represented.\n",
    "    Words that do not appear in the lexicon are considered as neutral.\n",
    "    :param sentence: a sentence whose polarity has to be classified.\n",
    "    \"\"\"\n",
    "    from nltk.corpus import opinion_lexicon\n",
    "    from nltk.tokenize import treebank\n",
    "\n",
    "    tokenizer = treebank.TreebankWordTokenizer()\n",
    "    pos_words = 0\n",
    "    neg_words = 0\n",
    "    tokenized_sent = [word.lower() for word in tokenizer.tokenize(sentence)]\n",
    "\n",
    "    for word in tokenized_sent:\n",
    "        if word in opinion_lexicon.positive():\n",
    "            pos_words += 1\n",
    "        elif word in opinion_lexicon.negative():\n",
    "            neg_words += 1\n",
    "\n",
    "\n",
    "    if pos_words > neg_words:\n",
    "        return 1 # positive\n",
    "    elif pos_words < neg_words:\n",
    "        return -1 # negative\n",
    "    elif pos_words == neg_words:\n",
    "        return 0 # neutral\n",
    "\n",
    "    return np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nErrmFxTygf"
   },
   "outputs": [],
   "source": [
    "filename = \"lyrics.csv\"\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opv2uSTlU_Ck"
   },
   "outputs": [],
   "source": [
    "df = df[df['lyrics']!='instrumental'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7s6etOWoU_vv"
   },
   "outputs": [],
   "source": [
    "genres = df.genre.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JPnc13fdV-3m"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9LVGThVVwfg"
   },
   "outputs": [],
   "source": [
    "lyrics = df['lyrics']\n",
    "vocab = set()\n",
    "for lyric in lyrics:\n",
    "  words = lyric.split()\n",
    "  for word in words:\n",
    "    vocab.add(lemmatizer.lemmatize(word.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9JUmkVFZVwWw",
    "outputId": "0390630d-c46e-473b-ef99-3a1d02db8a22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "837079"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lb6_6g2cVBzl"
   },
   "outputs": [],
   "source": [
    "def get_contexts_with_word(df2,word):\n",
    "#   df2 = df.sample(100)\n",
    "#   df2 = df2.dropna()\n",
    "#   df2['lyrics'] = df2['lyrics'].replace(r'\\n', ' ', regex = True)\n",
    "  r = []\n",
    "  ind = 0\n",
    "  indices = []\n",
    "  \n",
    "  for i,row in df2.iterrows():\n",
    "#     print(row)      \n",
    "      l = row['lyrics']\n",
    "      if (any(c.isalpha() for c in l) and detect(l) == 'en'):\n",
    "          r.append(l.split())\n",
    "          indices.append(row['index'])\n",
    "      ind += 1\n",
    "\n",
    "\n",
    "  k = 15\n",
    "  she_phrases = []\n",
    "  she_ind = []\n",
    "  for i,v in enumerate(r):\n",
    "    for ngram in nltk.ngrams(v, 2*k+1, pad_left=True, pad_right=True, left_pad_symbol=\" \", right_pad_symbol = \" \"):\n",
    "      if ngram[k].lower() == word:\n",
    "          s = \" \".join(ngram)\n",
    "#           print('s', s)\n",
    "          she_phrases.append((indices[i],s))\n",
    "#           she_ind.append(indices)\n",
    "          \n",
    "  return (she_phrases,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0KVovkeiZ696"
   },
   "outputs": [],
   "source": [
    "woman_insults = pd.read_csv('woman_insults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G51g56tAcaFS"
   },
   "outputs": [],
   "source": [
    "w_insults = woman_insults['insults'].tolist()\n",
    "for i in range(len(w_insults)):\n",
    "  w_insults[i] = w_insults[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_lko_s2d_LW"
   },
   "outputs": [],
   "source": [
    "def get_sentiment_word_context(df2,contextp,word):\n",
    "  for loc,context in contextp:\n",
    "    context = context\n",
    "    score_sn = 0\n",
    "    score_swn_pos = 0\n",
    "    sn = SenticNet()\n",
    "    score_swn_neg = 0\n",
    "    count_swn = 0\n",
    "    score_hi_pos = 0\n",
    "    score_hi_neg = 0\n",
    "#     print(context)\n",
    "    for word in context.split():\n",
    "      # Sentic Net\n",
    "      try:\n",
    "        sentics = sn.sentics(word)\n",
    "        score_sn_sensitivity = abs(float(sentics['sensitivity']))\n",
    "        score_sn_pleasantness = float(sentics['pleasantness'])\n",
    "      except:\n",
    "        score_sn_sensitivity = 0\n",
    "        score_sn_pleasantness = 0\n",
    "  #     Sentic Word Net\n",
    "      word_swn_lemmas = wn.synsets(word)\n",
    "      if not not word_swn_lemmas:\n",
    "        ans = swn.senti_synset(word_swn_lemmas[0].name()) #using 0 index because\n",
    "        score_swn_pos += ans.pos_score()\n",
    "        score_swn_neg += ans.neg_score()\n",
    "        count_swn += 1\n",
    "      # HI Score\n",
    "      if word in pos_hi_dict.keys():\n",
    "        score_hi_pos += pos_hi_dict[word]\n",
    "      if word in neg_hi_dict.keys():\n",
    "        score_hi_neg += neg_hi_dict[word]\n",
    "\n",
    "    (score_esn_anger, score_esn_disgust, score_esn_sad, score_esn_joy, score_esn_fear) = esn(context)\n",
    "#     print(score_esn_anger, score_esn_disgust, score_esn_sad, score_esn_joy, score_esn_fear)\n",
    "    df2.at[loc,'score_esn_anger'] += score_esn_anger\n",
    "    df2.at[loc,'score_esn_sad'] += score_esn_sad\n",
    "    df2.at[loc,'score_esn_joy'] += score_esn_joy\n",
    "    df2.at[loc,'score_esn_fear'] += score_esn_fear\n",
    "    df2.at[loc,'score_esn_disgust'] += score_esn_disgust\n",
    "    df2.at[loc,'score_hi_neg'] += score_hi_neg\n",
    "    df2.at[loc,'score_hi_pos'] += score_hi_pos\n",
    "    df2.at[loc,'score_swn_pos'] += score_swn_pos\n",
    "    df2.at[loc,'score_swn_neg'] += score_swn_neg\n",
    "    df2.at[loc,'score_sn_sensitivity'] += score_sn_sensitivity\n",
    "    df2.at[loc,'score_sn_pleasantness'] += score_sn_pleasantness\n",
    "  return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ql9bxVw6lf4M"
   },
   "outputs": [],
   "source": [
    "def add_cols(df):\n",
    "  sLen = len(df)\n",
    "  df['score_esn_anger'] = 0\n",
    "  df['score_esn_sad'] = 0\n",
    "  df['score_esn_joy'] = 0\n",
    "  df['score_esn_fear'] = 0\n",
    "  df['score_esn_disgust'] = 0\n",
    "  df['score_hi_neg'] = 0\n",
    "  df['score_hi_pos'] = 0\n",
    "  df['score_swn_pos'] = 0\n",
    "  df['score_swn_neg'] = 0\n",
    "  df['score_sn_sensitivity'] = 0\n",
    "  df['score_sn_pleasantness'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQwJnxq6jgsE"
   },
   "outputs": [],
   "source": [
    "she_phrases,index = get_contexts_with_word(df,\"she\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7mgKLylnA5Gr"
   },
   "outputs": [],
   "source": [
    "d_female = df.copy()\n",
    "add_cols(d_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YUmSWlObA19b"
   },
   "outputs": [],
   "source": [
    "d_female = get_sentiment_word_context(d_female,he_phrases,\"she\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6iawiQRO4mO"
   },
   "outputs": [],
   "source": [
    "df_male = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_H6fvJruG_w"
   },
   "outputs": [],
   "source": [
    "add_cols(df_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONfBM8RluOKW"
   },
   "outputs": [],
   "source": [
    "he_phrases,index = get_contexts_with_word(df,\"he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zc5IUI6HuZ9v"
   },
   "outputs": [],
   "source": [
    "d_male = get_sentiment_word_context(df_male,he_phrases,\"he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fejwR77EyBge"
   },
   "outputs": [],
   "source": [
    "d_female = pd.to_csv('she_sentiment.csv')\n",
    "d_male = pd.to_csv('he_sentiment.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bias using sentiment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
